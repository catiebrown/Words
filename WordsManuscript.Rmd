---
title             : "Ambiguous Words"
shorttitle        : "AMBIGUOUS WORDS"

author: 
  - name          : "Nicholas R. Harp"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "nharp@huskers.unl.edu"
  - name          : "Catherine C. Brown"
    affiliation   : "1"
  - name          : "Maital Neta"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Nebraska-Lincoln"

authornote: |
  Nicholas R. Harp, Department of Psychology, Center for Brain, Biology, and Behavior, University of Nebraska-Lincoln
  Catherine C. Brown, Department of Psychology, Center for Brain, Biology, and Behavior, University of Nebraska-Lincoln
  Maital Neta, Department of Psychology, Center for Brain, Biology, and Behavior, University of Nebraska-Lincoln

abstract: |
  We found some ambiugous words.
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "ambiguity"
wordcount         : "X"

bibliography      : ["CANLab_UNL.bib"] 

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences, include = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

# Set root directory
cbpath <- '~/Documents/GitHub/Words/'
nhpath <- '~/Documents/Nick-Grad/Neta_Lab/Words/'
path <- cbpath
# knitr::opts_knit$set(root.dir = path)


### load necessary libraries ###
#library(tidyverse)
library(readxl)
library(openxlsx)
library(Rmisc)
library(broom)

source("data_cleaning_nh.R")
```
# Introduction
We wanted to identify ambiguous words. Mention in this section why we also generated clearly positive and negative words.

# Study 1: Pilot 

## Methods

### Participants
Workers on Amazon's Mchanical Turk (MTurk) were invted to participate in an eligibility screener with the option to earn a bonus if they met the requirements and completed the entire study. The Workers clicked a hyperlink that directed them to the study. The screener task included demographic questions and one block of word ratings that included 5 instances of the word "negative" and 5 instances of the word "positive" (see Procedure below for full details). Workers were invited to complete the entire study if they indicated that they were over 18 years old, had English as their native language, had no history of psychological or neurological disorder, and correctly rated the words "positive" and "negative" as positive or negative with at least 80% accuracy. Of the 145 Workers who completed the screener, `r length(participants)` met the eligibility requirements, and `r length(final.participant)` (`r sum(str_count(demog$sex, "Female"))/length(final.participant)*100`% female, `r sum(str_count(demog$sex, "Male"))/length(final.participant) *100`% male) chose to complete the entire study. The final sample was `r sum(str_count(demog$race, "Asian"))/length(final.participant) *100`% Asian, `r sum(str_count(demog$race, "Black - not of Hispanic Origin"))/length(final.participant) *100`% Black, `r sum(str_count(demog$race, "Hispanic or Latino"))/length(final.participant) *100`% Hispanic or Latino, `r sum(str_count(demog$race, "White - not of Hispanic Origin"))/length(final.participant) *100`% White, and `r sum(str_count(demog$race, "Other (please specify)"))/length(final.participant) *100`% Other, with a mean(sd) age of `r mean(demog$age)`(`r sd(demog$age)`). 

NOTE: FOR SOME REASON THE "OTHER" PERCENTAGE IS NOT CALCULATING CORRECTLY

### Material

#### Stimuli
We compiled an initial set of 59 words that we believed had two distinct definitions, one clearly positive definition and one clearly negative definition. To create lists of clearly positive and clearly negative words, we first created a master list of words that were included in both the study by @warriner_norms_2013, for valence and arousal ratings, and the English Lexicon Project online word query [@balota_english_2007], for lexical characterisic measurements. We then elimiated any words with a mean arousal rating that was greater than 1 standard deviation away from the mean arousal of the list of 59 ambiguous words. We classified "positive" words as those with a mean valence > 7 on the 1-9 scale used by @warriner_norms_2013; "negative" words had mean valence < 3. To ensure that all words shared similar lexical characteristics, we eliminated any words from the master list whose lexical characteristics did not fall within the minimum and maximum values of the 59 ambiguous words' lexical characteristics. The following were used for the cutoffs: length, the frequency of a word as reported by the Hyperspace Analogue to Language (HAL) study [@lund_producing_1996], the log of HAL frequency, number of phonemes, number of syllables, number of morphemes, lexical decision reaction time and accuracy, and naming reaction time and accuracy. The final list of pilot words included 59 ambiguous, 267 positive, and 304 negative words. 

All of the calculations described in this section were scripted using R version `r getRversion()` and are available in the **Supplementary Information**. 

#### Software
All tasks were created and presented using Gorilla Experiment Builder [@anwyl-irvine_gorilla_2019]. The study was only accessible to participants using a computer (not a phone or tablet) within the United States.

### Screener and word rating task 
After giving informed consent, participants first answered demographic questions about their gender, age, race, native language, and whether they had ever been diagnosed with a psychological or neurological disorder. They  were thenshown a brief self-guided instructional walkthrough of the task before completing the screener. 

Using a random seed, we selected 20 positive and 20 negative words from the final pilot list for use in the screener task. These 40 words, along with 5 instances of the word "positive" and 5 instances of the word "negative" were presented randomly, one at a time, each following a 250 ms fixation cross. Each word remained on screen until the participant indicated that they thought it was positive or negative by pressing A or L on their keyboard (key pairing randomized across participants). If no response was made after 2000ms, a reminder appeared on screen, "Please respond as quickly as you can! A = POSITIVE. L = NEGATIVE." Participants who rated the words "positive" and "negative" with less than 80% accuracy were compensated for their time but were not invited to complete the rest of the study. Participants were also excluded at this point if they indicated that they were younger than 18, that English was not their native language, or that they had been diagnosed with a psychological or neurological disorder. 

The remaining 590 words from the final pilot list were randomly presented across 10 blocks of 59 words using the same button-press procedure as the screener block. 

## Results
Trials with a response time faster than 250ms were removed from the data prior to analysis, as well as trials with a reaction time greater than 3 SDs above the mean reaction time averaged across all trials. 

We assessed average reaction time to identify the ambiguous words within the range of 35%-65% average negative rating, suggesting low response consensus. Previous work has shown that ambiguous faces and images are associated with longer reaction times in a forced-choice valence classification task (CITE). **Figure 1a** shows that 29 amibugous, 5 negative, and 6 positive words surpassed a reaction time threshold of 875ms (Why did we use 875? Just visual inspection?). These 40 words were considered for inclusion in a final list of ambiguous words. We removed 7 words that did not have both a clearly positive and clearly negative definition ("recession", "faceless", "headstone", "inherit", "abundant", "cosmic", "receive"), as well as 1 word that was redundant to another ambiguous word that we included ("courtroom"), resulting in a final list of 32 ambiguous words.

As shown in **Figure 1b**, visual inspection of the average valence ratings revealed two distinct groups of words with high response consensus: one with a clearly negative meaning (n = 18, mean valence rating > 75% negative) and one with a clearly positive meaning (n = 20, mean valence rating < 10% negative). We removed the words "positive" and "negative" from each list (explain). Because the valence bias task requires an equal number of ambiguous (50%) and clearly-valenced (25% positive, 25% negative) stimuli, we included the 16 words with the fastest reaction time for the positive and negative word lists, respectively. 

```{r, fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure: Here is a really important caption."}

### figure 1

```

## Study 1 Discussion
Study 1 generated a list of 32 ambigous words, as well as 16 positive and 16 negative words, for use in determining whether valence bias generalizes to verbal ambiguity. Study 2 aimed to test this by comparing ratings of word to ratings of well-validated stimuli sets consisting of faces and scenes.

# Study 2: Comparison of words with valence bias and IPANAT

## Methods

### Participants

### Material

#### Stimuli
##### Valence Bias with Words
##### Valence Bias with Faces
##### Valence Bias with IAPS
##### IPANAT

#### Software

### Procedure
#### Valence Bias Tasks
#### IPANAT

### Data analysis
#### Valence Bias Tasks
#### IPANAT

## Results

### Subjective ratings
##### Valence Bias with Words
##### Valence Bias with Faces
##### Valence Bias with IAPS
##### IPANAT

### Reaction times
##### Valence Bias with Words
##### Valence Bias with Faces
##### Valence Bias with IAPS
##### IPANAT

### Relationships between the measures

# Discussion
We did this study.

\newpage

# References
```{r create_r-references}
r_refs(file = "CANLab_UNL.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage

# Supplementary Information

## Study 1 Stimuli
Insert link to repository for 'pick_words.R' 

